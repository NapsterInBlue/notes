<!DOCTYPE html>
<html lang="en">

<head>

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66582-32"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-66582-32');
    </script>

    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<meta property="og:title" content="Interpretability: Find the Essence of Filters" />
<meta property="og:description" content="%pylab inline import tensorflow as tf tf.logging.set_verbosity(tf.logging.ERROR) from keras import backend as K Populating the interactive namespace from numpy and matplotlib Using TensorFlow backend.  Recall the general architecture of the VGG model (Notes here).
from IPython.display import Image Image(&#39;images/vgg_long.png&#39;) If we load up the keras implementation of the weights (ignoring the top layers, as we&rsquo;re not actually going to be predicting anything)
from keras.applications import VGG16 model = VGG16(weights=&#39;imagenet&#39;, include_top=False) &hellip; we can see that it&rsquo;s very large." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://napsterinblue.github.io/notes/machine_learning/computer_vision/interpretable_filter_essence/" />



<meta property="article:published_time" content="2019-08-05T00:00:00&#43;00:00"/>

<meta property="article:modified_time" content="2019-08-05T00:00:00&#43;00:00"/>











<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Interpretability: Find the Essence of Filters"/>
<meta name="twitter:description" content="%pylab inline import tensorflow as tf tf.logging.set_verbosity(tf.logging.ERROR) from keras import backend as K Populating the interactive namespace from numpy and matplotlib Using TensorFlow backend.  Recall the general architecture of the VGG model (Notes here).
from IPython.display import Image Image(&#39;images/vgg_long.png&#39;) If we load up the keras implementation of the weights (ignoring the top layers, as we&rsquo;re not actually going to be predicting anything)
from keras.applications import VGG16 model = VGG16(weights=&#39;imagenet&#39;, include_top=False) &hellip; we can see that it&rsquo;s very large."/>
<meta name="generator" content="Hugo 0.40.3" />

    
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "headline": "Interpretability: Find the Essence of Filters",
  "url": "https://napsterinblue.github.io/notes/machine_learning/computer_vision/interpretable_filter_essence/",
  "wordCount": "922",
  "datePublished": "2019-08-05T00:00:00&#43;00:00",
  "dateModified": "2019-08-05T00:00:00&#43;00:00",
  "author": {
    "@type": "Person",
    "name": ""
  }
}
</script> 

    <title>Interpretability: Find the Essence of Filters</title>

    
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css" integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb"
        crossorigin="anonymous">

    
    <link href="https://napsterinblue.github.io/notes/css/custom.css" rel="stylesheet">
    <link href="https://napsterinblue.github.io/notes/css/syntax.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Muli:400,500,700" rel="stylesheet">

    <link href="" rel="alternate" type="application/rss+xml" title="Data Science Notes" />

    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>

</head>

<body>

    <nav class="navbar navbar-expand-sm fixed-top">
        <div class="container-fluid">
            <a class="navbar-brand" href="https://napsterinblue.github.io">Movies, Metrics, Musings</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent"
                aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="nav navbar-nav mr-auto"></ul>
                <ul class="nav navbar-nav">
                    <li><a href="https://napsterinblue.github.io/pages/about.html" title="About">About</a></li>
                    <li><a href="https://napsterinblue.github.io/archives.html" title="Archive">Archive</a></li>
                    <li><a href="https://napsterinblue.github.io/pages/resources.html" title="Resources">Resources</a></li>
                    <li><a href="https://napsterinblue.github.io/notes/" title="Notes">My Notes</a></li>

                </ul>
            </div>
        </div>
    </nav>


    
    <div class="container">
        <div class="row">
            <div class="col-sm-12">

                 


<article>
  <div class="technical_note">
  <header>
    <h1 class="technical_note_title">Interpretability: Find the Essence of Filters</h1>
    <div class="technical_note_date">
      <time datetime=" 2019-08-05T00:00:00Z "> 05 Aug 2019</time>
    </div>
  </header>
  <div class="content">
  

<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="o">%</span><span class="n">pylab</span> <span class="n">inline</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">set_verbosity</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">backend</span> <span class="k">as</span> <span class="n">K</span></code></pre></div>
<pre><code>Populating the interactive namespace from numpy and matplotlib


Using TensorFlow backend.
</code></pre>

<p>Recall the general architecture of the VGG model (<a href="https://napsterinblue.github.io/notes/machine_learning/computer_vision/vgg/">Notes here</a>).</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="n">Image</span><span class="p">(</span><span class="s1">&#39;images/vgg_long.png&#39;</span><span class="p">)</span></code></pre></div>
<p><img src="interpretable_filter_essence_3_0.png" alt="png" /></p>

<p>If we load up the <code>keras</code> implementation of the weights (ignoring the top layers, as we&rsquo;re not actually going to be predicting anything)</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">keras.applications</span> <span class="kn">import</span> <span class="n">VGG16</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">VGG16</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="s1">&#39;imagenet&#39;</span><span class="p">,</span>
              <span class="n">include_top</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span></code></pre></div>
<p>&hellip; we can see that it&rsquo;s very large.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span></code></pre></div>
<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, None, None, 3)     0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, None, None, 64)    1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (None, None, None, 64)    36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, None, None, 64)    0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, None, None, 128)   73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (None, None, None, 128)   147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, None, None, 128)   0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, None, None, 256)   295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (None, None, None, 256)   590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (None, None, None, 256)   590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, None, None, 256)   0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, None, None, 512)   0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, None, None, 512)   0         
=================================================================
Total params: 14,714,688
Trainable params: 14,714,688
Non-trainable params: 0
_________________________________________________________________
</code></pre>

<p>This tutorial is meant to shed some light on the types of intermediate features that these hidden layers are looking at.</p>

<h2 id="methodology-overview">Methodology Overview</h2>

<p>Broadly, we know that training a Neural Network involves:</p>

<ol>
<li>Setting the model structure</li>
<li>Defining the loss</li>
<li>Minimizing that loss</li>
</ol>

<p>Which means that, crucially, the connections from layer to layer are differentiable and aggregate to some comprehensive loss function.</p>

<p>A good intermediate filter can:</p>

<ol>
<li>&ldquo;Spot a particular pattern in our image&rdquo;</li>
<li>Pass some information to the next layer that confirms &ldquo;yeah, I saw this pattern&rdquo;</li>
</ol>

<p>Over multiple training steps, this <em>information passed to the next layer</em> is adjusted by performing gradient descent to arrive at a series of weights that minimize this filter&rsquo;s contribution to the loss of the whole model.</p>

<p>The key mechanic we&rsquo;re going to be using here basically up-ends this whole idea. Instead of using the gradient to go from image to &ldquo;singular information value&rdquo; to minimize loss, we&rsquo;re going to <em>multiply by this gradient</em> to make adjustments to the image to <em>maximize loss</em>.</p>

<p>With enough repetition, this will give us an image with higher and higher loss for this filter, which translates in English to &ldquo;an image tailor made to activate this particular filter.&rdquo;</p>

<h3 id="visualizing-one-filter-of-one-layer">Visualizing One Filter of One Layer</h3>

<p>So to get a ground-level intuition for how this all works, we&rsquo;re going to zero in on <em>one</em> filter of <em>one</em> block.</p>

<p>Arbitrarily-chosen, we&rsquo;ve got:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">layer_name</span> <span class="o">=</span> <span class="s1">&#39;block3_conv1&#39;</span>
<span class="n">filter_index</span> <span class="o">=</span> <span class="mi">27</span></code></pre></div>
<p>And so we grab the layer that we&rsquo;re interested in examining</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">layer_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span><span class="o">.</span><span class="n">output</span></code></pre></div>
<p>It&rsquo;s got <code>256</code> different filters</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">layer_output</span><span class="o">.</span><span class="n">shape</span></code></pre></div>
<pre><code>TensorShape([Dimension(None), Dimension(None), Dimension(None), Dimension(256)])
</code></pre>

<p>So we&rsquo;ll just look at the filter we&rsquo;re interested in. We create an object called <code>loss</code>, which will be the outbound calculation for the loss that the network is trained to minimize.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">loss</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">layer_output</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="n">filter_index</span><span class="p">])</span></code></pre></div>
<p><strong>Note</strong>: This isn&rsquo;t the value of the loss&ndash; a cut-and-dry number&ndash; but instead, the <em>variable that will hold the value of the loss</em> as the network adjusts. We take the <code>mean</code> value to make the optimization more straight-forward.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="nb">type</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span></code></pre></div>
<pre><code>tensorflow.python.framework.ops.Tensor
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">loss</span><span class="o">.</span><span class="n">shape</span></code></pre></div>
<pre><code>TensorShape([])
</code></pre>

<p>Similarly, we define an object, <code>grads</code>, that will hold the value of the loss gradient, used in our repeated ascent step.</p>

<p>Looking closer, <code>K.gradients</code> evaluates our <code>loss</code> function (defined above), at the point defined by the <code>model.input</code>&ndash; in this case, an image of a consistent size. Per usual, we normalize that value, and the resulting <code>grads</code> object provides a gradient value across <code>(R, G, B)</code></p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">grads</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">variables</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="nb">input</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">grads</span> <span class="o">/=</span> <span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">grads</span><span class="p">)))</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">)</span>

<span class="n">grads</span><span class="o">.</span><span class="n">shape</span></code></pre></div>
<pre><code>TensorShape([Dimension(None), Dimension(None), Dimension(None), Dimension(3)])
</code></pre>

<p>Next, in the spirit of &ldquo;define the steps/placeholders&rdquo;, we&rsquo;ll use the <code>K.function</code> to define our iteration step.</p>

<p>Essentially, this lambda function will look for an image of our consistent size, and return <code>loss</code> and <code>grads</code> used to update our image at each step.</p>

<p>We&rsquo;ll take one step over an image of all black to initialize the values of <code>loss_value</code> and <code>grads_value</code>.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">iterate</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="nb">input</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">loss</span><span class="p">,</span> <span class="n">grads</span><span class="p">])</span>

<span class="n">loss_value</span><span class="p">,</span> <span class="n">grads_value</span> <span class="o">=</span> <span class="n">iterate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">))])</span></code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="nb">type</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span></code></pre></div>
<pre><code>tensorflow.python.framework.ops.Tensor
</code></pre>

<p>Then we create an image of just noise.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">input_img_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span> <span class="o">*</span> <span class="mi">20</span> <span class="o">+</span> <span class="mf">128.</span>

<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">input_img_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="mf">255.</span><span class="p">);</span></code></pre></div>
<p><img src="interpretable_filter_essence_29_0.png" alt="png" /></p>

<p>And loop over it a few hundred times, <em>adding</em> the gradients (dampened by <code>step</code>, our &ldquo;learning rate&rdquo;) as we go along</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">step</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">loss_value</span><span class="p">,</span> <span class="n">grads_value</span> <span class="o">=</span> <span class="n">iterate</span><span class="p">([</span><span class="n">input_img_data</span><span class="p">])</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_value</span><span class="p">)</span>
    
    <span class="n">input_img_data</span> <span class="o">+=</span> <span class="n">grads_value</span> <span class="o">*</span> <span class="n">step</span></code></pre></div>
<p>After many iterations, we can see the general pattern that this layer is looking for (clipping values outside of <code>[0.0, 1.0]</code>)</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">input_img_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="mf">255.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">));</span></code></pre></div>
<p><img src="interpretable_filter_essence_33_0.png" alt="png" /></p>

<h2 id="visualizing-many-filters-of-many-layers">Visualizing Many Filters of Many Layers</h2>

<p>Packaging the above code into a helper function called <code>generate_pattern</code>, we can easily plot out a filter in a specific layer</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">helpers</span></code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">helpers</span><span class="o">.</span><span class="n">generate_pattern</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;block3_conv1&#39;</span><span class="p">,</span> <span class="n">filter_index</span><span class="o">=</span><span class="mi">0</span><span class="p">));</span></code></pre></div>
<p><img src="interpretable_filter_essence_37_0.png" alt="png" /></p>

<p>Then borrowing once more from Chollet, we can do some clever loop magicâ„¢ and peek at several filters in each layer</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">helpers</span><span class="o">.</span><span class="n">generate_layer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;block1_conv1&#39;</span><span class="p">)</span></code></pre></div>
<p><img src="interpretable_filter_essence_39_0.png" alt="png" /></p>

<p>Look at how much more intricate the activations are in the later blocks of the Network</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">helpers</span><span class="o">.</span><span class="n">generate_layer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;block4_conv3&#39;</span><span class="p">)</span></code></pre></div>
<p><img src="interpretable_filter_essence_41_0.png" alt="png" /></p>

<p>This is immediately consistent with out recurring notion of &ldquo;increasing feature complexity, the deeper in the Network we go&rdquo;</p>

</div>
  <aside>
      <div class="bug_reporting">
          <h4>Find an error or bug?</h4>
          <p>Everything on this site is available on GitHub. Head to <a href='https://github.com/napsterinblue/notes/issues/new'>and submit a suggested change</a>. You can also message me directly on <a href='https://twitter.com/napsterinblue'>Twitter</a>.</p>
      </div>
      </aside>

    </div>
</article>




            </div>

        </div>
    </div>

    

    <footer class="footer text-center">
        <div class="container">
            <span class="text-muted">This project contains 185 pages and is available on <a href="https://github.com/napsterinblue/notes">GitHub</a>. Copyright &copy; NapsterInBlue, <time datetime="2018">2018</time>.</span>
        </div>
    </footer>

    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.3/umd/popper.min.js" integrity="sha384-vFJXuSJphROIrBnz7yo7oB41mKfc8JzQZiCq4NCceLEaO4IHwicKwpJf9c9IpFgh"
        crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/js/bootstrap.min.js" integrity="sha384-alpBpkh1PFOepccYVYDB4do5UnbKysX5WZXm3XxPqe5iKTfUKjNkCk9SaVuEZflJ"
        crossorigin="anonymous"></script>

</body>

</html>
