<!DOCTYPE html>
<html lang="en">

<head>

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66582-32"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-66582-32');
    </script>

    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<meta property="og:title" content="Logistic Regression Gradient Descent" />
<meta property="og:description" content="The Building Blocks Recall our equation for the Cost Function of a Logistic Regression
$\mathcal{L}(\hat{y}, y) = -\big(y\log\hat{y} &#43; (1-y)\log(1-\hat{y})\big)$
We use the weights, w, our inputs, x, and a bias term, b to get a vector z.
$z = w^{T} x &#43; b$
And we want this vector to be between 0 and 1, so we pipe it through a sigmoid function, to get our predictions.
$\hat{y} = \sigma(z)$" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://napsterinblue.github.io/notes/machine_learning/regression/logit_grad_descent/" />



<meta property="article:published_time" content="2018-08-07T00:00:00&#43;00:00"/>

<meta property="article:modified_time" content="2018-08-07T00:00:00&#43;00:00"/>











<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Logistic Regression Gradient Descent"/>
<meta name="twitter:description" content="The Building Blocks Recall our equation for the Cost Function of a Logistic Regression
$\mathcal{L}(\hat{y}, y) = -\big(y\log\hat{y} &#43; (1-y)\log(1-\hat{y})\big)$
We use the weights, w, our inputs, x, and a bias term, b to get a vector z.
$z = w^{T} x &#43; b$
And we want this vector to be between 0 and 1, so we pipe it through a sigmoid function, to get our predictions.
$\hat{y} = \sigma(z)$"/>
<meta name="generator" content="Hugo 0.40.3" />

    
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "headline": "Logistic Regression Gradient Descent",
  "url": "https://napsterinblue.github.io/notes/machine_learning/regression/logit_grad_descent/",
  "wordCount": "738",
  "datePublished": "2018-08-07T00:00:00&#43;00:00",
  "dateModified": "2018-08-07T00:00:00&#43;00:00",
  "author": {
    "@type": "Person",
    "name": ""
  }
}
</script> 

    <title>Logistic Regression Gradient Descent</title>

    
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css" integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb"
        crossorigin="anonymous">

    
    <link href="https://napsterinblue.github.io/notes/css/custom.css" rel="stylesheet">
    <link href="https://napsterinblue.github.io/notes/css/syntax.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Muli:400,500,700" rel="stylesheet">

    <link href="" rel="alternate" type="application/rss+xml" title="Data Science Notes" />

    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>

</head>

<body>

    <nav class="navbar navbar-expand-sm fixed-top">
        <div class="container-fluid">
            <a class="navbar-brand" href="https://napsterinblue.github.io">Movies, Metrics, Musings</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent"
                aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="nav navbar-nav mr-auto"></ul>
                <ul class="nav navbar-nav">
                    <li><a href="https://napsterinblue.github.io/pages/about.html" title="About">About</a></li>
                    <li><a href="https://napsterinblue.github.io/archives.html" title="Archive">Archive</a></li>
                    <li><a href="https://napsterinblue.github.io/pages/resources.html" title="Resources">Resources</a></li>
                    <li><a href="https://napsterinblue.github.io/notes/" title="Notes">My Notes</a></li>

                </ul>
            </div>
        </div>
    </nav>


    
    <div class="container">
        <div class="row">
            <div class="col-sm-12">

                 


<article>
  <div class="technical_note">
  <header>
    <h1 class="technical_note_title">Logistic Regression Gradient Descent</h1>
    <div class="technical_note_date">
      <time datetime=" 2018-08-07T00:00:00Z "> 07 Aug 2018</time>
    </div>
  </header>
  <div class="content">
  

<h2 id="the-building-blocks">The Building Blocks</h2>

<p>Recall our equation for the Cost Function of a Logistic Regression</p>

<p>$\mathcal{L}(\hat{y}, y) = -\big(y\log\hat{y} + (1-y)\log(1-\hat{y})\big)$</p>

<p>We use the weights, <code>w</code>, our inputs, <code>x</code>, and a bias term, <code>b</code> to get a vector <code>z</code>.</p>

<p>$z = w^{T} x + b$</p>

<p>And we want this vector to be between <code>0</code> and <code>1</code>, so we pipe it through a sigmoid function, to get our predictions.</p>

<p>$\hat{y} = \sigma(z)$</p>

<p>We refer to the sigmoid function that runs over all of our values as the <em>activation function</em>, so for shorthand, we&rsquo;ll say</p>

<p>$a = \hat{y}$</p>

<p>And thus</p>

<p>$\mathcal{L}(a, y) = -\big(y\log a + (1-y)\log(1 - a)\big)$</p>

<h2 id="gradient-descent">Gradient Descent</h2>

<p>So if our aim is to minimize our overall cost, we need to lean on some calculus.</p>

<p>Idea here is that we&rsquo;re going to take incremental steps across the inputs of cost function&ndash; the weights and bias term, taking <code>x</code> as given. Which means that we want work out the derivative of the cost function <em>with respect to those terms</em>.</p>

<h2 id="finding-the-derivatives">Finding the Derivatives</h2>

<p>Looking at the chain of execution to arrive at our cost function, we have:</p>

<p>Our <code>z</code> as an intermediate value, generated as a function of <code>w</code>, <code>X</code>, and <code>b</code></p>

<p>$z = w^{T}x + b$</p>

<p><code>a</code>, which is a function of <code>z</code>, applied with the our standard sigmoid function</p>

<p>$a = \hat{y} = \sigma(z) = \frac{1}{1 + e^{-z}}$</p>

<p>Finally, <em>Loss</em> is a function of <code>y</code>, or true values, and <code>a</code> (all of <em>its</em> dependencies)</p>

<p>$\mathcal{L}(a, y) = -\big(y\log a + (1-y)\log(1 - a)\big)$</p>

<p>We&rsquo;re trying to Chain Rule our way backwards, so we need to figure out all of the partial derivatives that impact this loss function.</p>

<h3 id="key-derivatives-to-take-as-given">Key Derivatives to take as Given</h3>

<p>Hand-wavy derivations, courtesy of the <em>Logistic Regression Gradient Descent</em> video during Week 2 of <a href="https://www.coursera.org/learn/neural-networks-deep-learning">Neural Networks and Deep Learning</a></p>

<h4 id="sigmoid-wrt-z">Sigmoid wrt <code>z</code></h4>

<p>$\frac{\delta a}{\delta z} = a (1 - a)$</p>

<h4 id="loss-function-wrt-a">Loss Function wrt <code>a</code></h4>

<p>$\frac{\delta \mathcal {L}}{\delta a} = -\frac{y}{a} + \frac{1-y}{1-a}$</p>

<h4 id="applying-the-chain-rule-to-our-loss-function">Applying the Chain Rule to our Loss Function</h4>

<p>$\frac{\delta \mathcal {L}}{\delta z} = \frac{\delta \mathcal {L}}{\delta a}\frac{\delta a}{\delta z}$</p>

<p>Substituting in</p>

<p>$\frac{\delta \mathcal {L}}{\delta z} = \big( -\frac{y}{a} + \frac{1-y}{1-a} \big) a(1-a)$</p>

<p>$\frac{\delta \mathcal {L}}{\delta z} = a-y$</p>

<h3 id="extrapolating-to-weights-and-bias">Extrapolating to weights and bias</h3>

<p>Assuming a simple formula for <code>z</code> of the form</p>

<p>$z = w_1 x_1 + w_2 x_2 + b$</p>

<p>We can apply the same Chain Rule logic as above</p>

<h4 id="w-1">w_1</h4>

<p>$\frac{\delta \mathcal{L}}{\delta w_1} = \frac{\delta \mathcal{L}}{\delta z} \frac{\delta z}{\delta w_1}$</p>

<p>Substituting again</p>

<p>$\frac{\delta \mathcal{L}}{\delta w_1} = (a - y) \frac{\delta z}{\delta w_1}$</p>

<p>$\frac{\delta \mathcal{L}}{\delta w_1} = (a - y) x_1$</p>

<h4 id="w-2">w_2</h4>

<p>Follows the exact same form</p>

<p>$\frac{\delta \mathcal{L}}{\delta w_2} = (a - y) x_2$</p>

<h4 id="bias">Bias</h4>

<p>Derivative just goes to 1 and cancels out the term</p>

<p>$\frac{\delta \mathcal{L}}{\delta b} = (a - y)$</p>

<h2 id="at-scale">At Scale</h2>

<p>So far we&rsquo;ve been showing the cost of <em>one</em> training example. Now, when you consider all <code>m</code> training examples, your Cost Function looks like</p>

<p>$J(w, b) = \frac{1}{m} \sum^{m}_{i=1} \mathcal{L}\big( a^{i}, y \big)$</p>

<p>And if you want to take the derivative of this, with respect to whatever, the fraction and the summation terms are going to get kicked out to the front, because mathâ„¢. This makes the calculation very tidy!</p>

<h3 id="the-descent-algorithm">The Descent Algorithm</h3>

<p>We set our parameters to <code>0</code>, by default</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">J</span><span class="p">,</span> <span class="n">dw_1</span><span class="p">,</span> <span class="n">dw_2</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="mi">0</span></code></pre></div>
<p>And then this loop happens <em>for each training iteration step</em></p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># one pass for each of the m training examples</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">sigma</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">J</span> <span class="o">+=</span> <span class="n">cost</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
    <span class="n">dz</span> <span class="o">+=</span> <span class="n">a</span> <span class="o">-</span> <span class="n">y</span>
    <span class="n">dw_1</span> <span class="o">+=</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">dz</span>
    <span class="n">dw_2</span> <span class="o">+=</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">dz</span>
    <span class="n">db</span> <span class="o">+=</span> <span class="n">dz</span>

<span class="c1"># handle the leading fraction in the cost function</span>
<span class="n">J</span> <span class="o">=</span> <span class="n">J</span> <span class="o">/</span> <span class="n">m</span>
<span class="n">dw_1</span> <span class="o">=</span> <span class="n">dw_1</span> <span class="o">/</span> <span class="n">m</span>
<span class="n">dw_2</span> <span class="o">=</span> <span class="n">dw_2</span> <span class="o">/</span> <span class="n">m</span>

<span class="c1"># adjust weights by learning rate</span>
<span class="n">w_1</span> <span class="o">=</span> <span class="n">w_1</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">dw_1</span>
<span class="n">w_2</span> <span class="o">=</span> <span class="n">w_2</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">dw_2</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">b</span></code></pre></div>
<p>All of this looping is, of course, wildly inefficient. Which is why we vectorize.</p>

<h3 id="vectorized-implementation">Vectorized Implementation</h3>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">J</span> <span class="o">=</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Our iterations</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">sigma</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>

<span class="n">dZ</span> <span class="o">=</span> <span class="n">A</span> <span class="o">-</span> <span class="n">Y</span>

<span class="n">dw</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dZ</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">db</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dZ</span><span class="p">)</span>

<span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">dw</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">db</span></code></pre></div>
<p>Note: If we want to run <code>1000</code> iterations, we&rsquo;d still have to wrap the third line down in a <code>for</code> loop</p>

</div>
  <aside>
      <div class="bug_reporting">
          <h4>Find an error or bug?</h4>
          <p>Everything on this site is available on GitHub. Head to <a href='https://github.com/napsterinblue/notes/issues/new'>and submit a suggested change</a>. You can also message me directly on <a href='https://twitter.com/napsterinblue'>Twitter</a>.</p>
      </div>
      </aside>

    </div>
</article>




            </div>

        </div>
    </div>

    

    <footer class="footer text-center">
        <div class="container">
            <span class="text-muted">This project contains 185 pages and is available on <a href="https://github.com/napsterinblue/notes">GitHub</a>. Copyright &copy; NapsterInBlue, <time datetime="2018">2018</time>.</span>
        </div>
    </footer>

    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.3/umd/popper.min.js" integrity="sha384-vFJXuSJphROIrBnz7yo7oB41mKfc8JzQZiCq4NCceLEaO4IHwicKwpJf9c9IpFgh"
        crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/js/bootstrap.min.js" integrity="sha384-alpBpkh1PFOepccYVYDB4do5UnbKysX5WZXm3XxPqe5iKTfUKjNkCk9SaVuEZflJ"
        crossorigin="anonymous"></script>

</body>

</html>
