<!DOCTYPE html>
<html lang="en">

<head>

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66582-32"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-66582-32');
    </script>

    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<meta property="og:title" content="Partial Least Squares" />
<meta property="og:description" content="As mentioned in our notebook on Principal Component Analysis, the chief goal of a dimension reduction technique is to express the observations of our p-dimensional dataset, X as a linear combination of m-dimensional vectors (m &lt; p), Z, using a mapping optimized &ldquo;to explain the most variation in our data.&rdquo;
But whereas PCA is an unsupervised method that involves figuring out how to explain variation in X, the Partial Least Squares method introduces a supervised alternative and considers our target, Y, in the dimension reduction." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://napsterinblue.github.io/notes/machine_learning/regression/partial_least_squares/" />



<meta property="article:published_time" content="2019-09-20T00:00:00&#43;00:00"/>

<meta property="article:modified_time" content="2019-09-20T00:00:00&#43;00:00"/>











<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Partial Least Squares"/>
<meta name="twitter:description" content="As mentioned in our notebook on Principal Component Analysis, the chief goal of a dimension reduction technique is to express the observations of our p-dimensional dataset, X as a linear combination of m-dimensional vectors (m &lt; p), Z, using a mapping optimized &ldquo;to explain the most variation in our data.&rdquo;
But whereas PCA is an unsupervised method that involves figuring out how to explain variation in X, the Partial Least Squares method introduces a supervised alternative and considers our target, Y, in the dimension reduction."/>
<meta name="generator" content="Hugo 0.40.3" />

    
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "headline": "Partial Least Squares",
  "url": "https://napsterinblue.github.io/notes/machine_learning/regression/partial_least_squares/",
  "wordCount": "481",
  "datePublished": "2019-09-20T00:00:00&#43;00:00",
  "dateModified": "2019-09-20T00:00:00&#43;00:00",
  "author": {
    "@type": "Person",
    "name": ""
  }
}
</script> 

    <title>Partial Least Squares</title>

    
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css" integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb"
        crossorigin="anonymous">

    
    <link href="https://napsterinblue.github.io/notes/css/custom.css" rel="stylesheet">
    <link href="https://napsterinblue.github.io/notes/css/syntax.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Muli:400,500,700" rel="stylesheet">

    <link href="" rel="alternate" type="application/rss+xml" title="Data Science Notes" />

    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>

</head>

<body>

    <nav class="navbar navbar-expand-sm fixed-top">
        <div class="container-fluid">
            <a class="navbar-brand" href="https://napsterinblue.github.io">Movies, Metrics, Musings</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent"
                aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="nav navbar-nav mr-auto"></ul>
                <ul class="nav navbar-nav">
                    <li><a href="https://napsterinblue.github.io/pages/about.html" title="About">About</a></li>
                    <li><a href="https://napsterinblue.github.io/archives.html" title="Archive">Archive</a></li>
                    <li><a href="https://napsterinblue.github.io/pages/resources.html" title="Resources">Resources</a></li>
                    <li><a href="https://napsterinblue.github.io/notes/" title="Notes">My Notes</a></li>

                </ul>
            </div>
        </div>
    </nav>


    
    <div class="container">
        <div class="row">
            <div class="col-sm-12">

                 


<article>
  <div class="technical_note">
  <header>
    <h1 class="technical_note_title">Partial Least Squares</h1>
    <div class="technical_note_date">
      <time datetime=" 2019-09-20T00:00:00Z "> 20 Sep 2019</time>
    </div>
  </header>
  <div class="content">
  

<p>As mentioned in <a href="https://napsterinblue.github.io/notes/stats/techniques/pca/">our notebook on Principal Component Analysis</a>, the chief goal of a dimension reduction technique is to express the observations of our <code>p</code>-dimensional dataset, <code>X</code> as a linear combination of <code>m</code>-dimensional vectors (<code>m &lt; p</code>), <code>Z</code>, using a mapping optimized &ldquo;to explain the most variation in our data.&rdquo;</p>

<p>But whereas PCA is an unsupervised method that involves figuring out how to explain variation in <code>X</code>, the Partial Least Squares method introduces a <em>supervised</em> alternative and considers our target, <code>Y</code>, in the dimension reduction.</p>

<p>Or to quote ISL:</p>

<blockquote>
<p>Roughly speaking, the PLS approach attempts to find directions that help explain both the response and the predictors.</p>
</blockquote>

<h2 id="intuition">Intuition</h2>

<p>Recall that the general idea of PCA is to:</p>

<ul>
<li>Find an axis that explains the most variation in <code>X</code></li>
<li>Re-orient our data relative to this new axis</li>
<li>Repeat until we reach some desired &ldquo;explained variation&rdquo; threshold</li>
</ul>

<p>PLS follows a similar approach, but instead begins with basically a Least Squares regression on <code>Y</code></p>

<p>After we normalize our data, the algorithm can be described as thefollowing (borrowed from <a href="https://web.stanford.edu/class/stats202/content/lec15.pdf">this Stanford lecture</a>):</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="n">Image</span><span class="p">(</span><span class="s1">&#39;images/pls_alg.PNG&#39;</span><span class="p">)</span></code></pre></div>
<p><img src="partial_least_squares_3_0.png" alt="png" /></p>

<p>Decrypting this a bit, we start by taking a linear regression on <code>y</code> to get our coefficients <code>theta_j1</code>.</p>

<p>We use this to transform <code>X_j</code> into <code>y_hat = Z_1</code>&ndash; our prediction vector&ndash; by taking a linear combination. As with any regression, we expect to see a bunch of residual prediction errors between <code>y_hat</code> and <code>y</code>.</p>

<p>Flipping this, <code>X_j^(2)</code> will represent the &ldquo;missing information&rdquo; that we have for trying to predict our original <code>X_j</code> values using our new mapping <code>Z_1</code>.</p>

<p>At this point, we want to continue in the PCA fashion of &ldquo;find the axis that explains <em>the next-most variance</em>.&rdquo; If we use these &ldquo;missing information residuals&rdquo;, <code>X_j^(2)</code> to try and predict <code>y</code>, we have a new set of coefficients <code>theta_j2</code> that combine with <code>X_j^(2)</code> to make our second mapping, <code>Z_2</code>.</p>

<p>We continue in this fashion, using the residuals of &ldquo;missing information&rdquo; to mine more axes</p>

<h2 id="multivariate-y">Multivariate <code>Y</code></h2>

<p>One important feature of PLS worth mentioning is that it allows us to not only include <code>Y</code> in our dimension reduction scheme, it also neatly extends to <em>mutivariate</em> dimensions of <code>Y</code>.</p>

<p>In a sense, you can conceptualize this as doing a sort of PCA on both <code>X</code> and <code>Y</code>, then searching for the latent structure of <code>X</code> best explains the latent structure of <code>Y</code>.</p>

<p><a href="https://www.youtube.com/watch?v=WKEGhyFx0Dg">This video</a> does a good job highlighting the idea, visually</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">Image</span><span class="p">(</span><span class="s1">&#39;images/pls_multivariate.PNG&#39;</span><span class="p">)</span></code></pre></div>
<p><img src="partial_least_squares_7_0.png" alt="png" /></p>

<p>To put this another way, if we can find some representation <code>U</code> in <code>Y</code> that explains most of the variation in our target space, then <code>T</code>, our representation of <code>X</code>, will be optimized to maximize the correlation between <code>U</code> and <code>T</code>, as described in <a href="https://www.youtube.com/watch?v=AxmqUKYeD-U">this video</a></p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">Image</span><span class="p">(</span><span class="s1">&#39;images/pls_multivariate_cross.PNG&#39;</span><span class="p">)</span></code></pre></div>
<p><img src="partial_least_squares_9_0.png" alt="png" /></p>

<p>Finally, <a href="https://www.youtube.com/watch?v=Qt3Vv5KsnpA">the first 2 minutes of this video</a> do an <em>exceptional job</em> illustrating the incremental, simultaneous fitting of <code>T</code> and <code>U</code> and should be watched in excess of 100 times, IMO.</p>

</div>
  <aside>
      <div class="bug_reporting">
          <h4>Find an error or bug?</h4>
          <p>Everything on this site is available on GitHub. Head to <a href='https://github.com/napsterinblue/notes/issues/new'>and submit a suggested change</a>. You can also message me directly on <a href='https://twitter.com/napsterinblue'>Twitter</a>.</p>
      </div>
      </aside>

    </div>
</article>




            </div>

        </div>
    </div>

    

    <footer class="footer text-center">
        <div class="container">
            <span class="text-muted">This project contains 185 pages and is available on <a href="https://github.com/napsterinblue/notes">GitHub</a>. Copyright &copy; NapsterInBlue, <time datetime="2018">2018</time>.</span>
        </div>
    </footer>

    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.3/umd/popper.min.js" integrity="sha384-vFJXuSJphROIrBnz7yo7oB41mKfc8JzQZiCq4NCceLEaO4IHwicKwpJf9c9IpFgh"
        crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/js/bootstrap.min.js" integrity="sha384-alpBpkh1PFOepccYVYDB4do5UnbKysX5WZXm3XxPqe5iKTfUKjNkCk9SaVuEZflJ"
        crossorigin="anonymous"></script>

</body>

</html>
